{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6edbea2d",
   "metadata": {},
   "source": [
    "# Preprocessing of Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ea043",
   "metadata": {},
   "source": [
    "The preprocessing script contains a section with all preprocessing functions. These should be easily adaptable to working with text that isn't tweets. To accomodate for both English and Danish language some functions have additional arguments 'stops' and 'langStemmer' so we can use the same function in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f50d2c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\stine\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing packages \n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "#Detect langugage\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# for Danish\n",
    "from nltk.stem.snowball import DanishStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78a2675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ DATA \n",
    "data = pd.read_csv(\"C:/Users/stine/OneDrive/melchior_job/female_pol_data/comments_all_female_updated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def0c6c8",
   "metadata": {},
   "source": [
    "### Dropping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e0c58b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping ALL duplicate values\n",
    "data.drop_duplicates(subset = \"comment_id\", #'comment_id' for comments and 'id' for posts\n",
    "                     keep = 'first', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c8da6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMENT only: remove deleted comments\n",
    "data = data[data['comment_body'] != '[deleted]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "294ae9c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'title'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f85a709bb3e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# POSTS only: concatenate title and body (only for post-csvs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"concatenated_text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"title\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"body\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m\". \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_rep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'title'"
     ]
    }
   ],
   "source": [
    "# POSTS only: concatenate title and body (only for post-csvs)\n",
    "data[\"concatenated_text\"]= data[\"title\"].str.cat(data[\"body\"], sep =\". \", na_rep = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7a9764",
   "metadata": {},
   "source": [
    "# Language Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "033fb89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "da    3916\n",
       "no     347\n",
       "en      74\n",
       "sl      55\n",
       "de      31\n",
       "sv      23\n",
       "nl      13\n",
       "et      12\n",
       "sk      11\n",
       "af      11\n",
       "id      11\n",
       "pt       9\n",
       "sw       9\n",
       "it       9\n",
       "tr       8\n",
       "so       8\n",
       "tl       7\n",
       "cy       6\n",
       "hr       6\n",
       "hu       5\n",
       "fi       4\n",
       "pl       4\n",
       "fr       4\n",
       "sq       4\n",
       "es       3\n",
       "vi       2\n",
       "ca       2\n",
       "ro       1\n",
       "cs       1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in data.iterrows():\n",
    "    try:\n",
    "        data.loc[index, 'lang'] = detect(row['comment_body']) # 'comment_body' for comments 'concatenated_text' for posts\n",
    "    except LangDetectException:\n",
    "        data.loc[index, 'lang'] = None\n",
    "\n",
    "\n",
    "data['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4852e80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continue only with the scandinavian tweets \n",
    "tweets_da = data[data.lang.isin(['da', 'no', 'sv'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42930ac6",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65880189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(corpus, stops):\n",
    "    no_urls = [re.sub(r\"http\\S+\", \"\", text) for text in corpus] # remove links\n",
    "    only_letters = [re.sub(\"(#[A-Za-z]+)|(@[A-Za-z]+)|([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", ' ' , text) for text in no_urls] # remove numbers\n",
    "    only_letters = [text.replace('\\n', ' ') for text in only_letters] # remove newline characters\n",
    "    lowercased_tweets = [text.lower().split() for text in only_letters] # lowercase all words\n",
    "    no_stopwords = [[w for w in text if not w in stops] for text in lowercased_tweets] # remove stopwords\n",
    "    tweets = [\" \".join(text) for text in no_stopwords] # join the lowered and cleaned words\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de1632f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization of text\n",
    "def tokenize_text(tweets):\n",
    "    tokens = [word_tokenize(text) for text in tweets] # tokenizes every tweet using nltk\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d91b7485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization of text using the nlp() method from SpaCy.\n",
    "def lemmatize_text(sent):\n",
    "    lemmas = [x.lemma_ for x in nlp(sent)]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59a75b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming of text using nltk\n",
    "def stem_text(sent, langStemmer):\n",
    "    stemmer = langStemmer\n",
    "    stem = [stemmer.stem(token) for token in sent]\n",
    "    return stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b002d2",
   "metadata": {},
   "source": [
    "# Danish Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95f5cf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['og', 'i', 'jeg', 'det', 'at', 'en', 'den', 'til', 'er', 'som', 'på', 'de', 'med', 'han', 'af', 'for', 'ikke', 'der', 'var', 'mig', 'sig', 'men', 'et', 'har', 'om', 'vi', 'min', 'havde', 'ham', 'hun', 'nu', 'over', 'da', 'fra', 'du', 'ud', 'sin', 'dem', 'os', 'op', 'man', 'hans', 'hvor', 'eller', 'hvad', 'skal', 'selv', 'her', 'alle', 'vil', 'blev', 'kunne', 'ind', 'når', 'være', 'dog', 'noget', 'ville', 'jo', 'deres', 'efter', 'ned', 'skulle', 'denne', 'end', 'dette', 'mit', 'også', 'under', 'have', 'dig', 'anden', 'hende', 'mine', 'alt', 'meget', 'sit', 'sine', 'vor', 'mod', 'disse', 'hvis', 'din', 'nogle', 'hos', 'blive', 'mange', 'ad', 'bliver', 'hendes', 'været', 'thi', 'jer', 'sådan', 'paa', 'saa', 'vaere', 'rt', 'ogsaa', 'faa', 'faar', 'nok', 'mt', 'gt']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tweets_da' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-de799d05ddd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# exchange special characters æ, ø, å for their 'international' equivalents.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtweets_da\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ø'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'oe'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweets_da\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comment_body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# concatenated_text for posts and comment_body for comments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mtweets_da\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'æ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ae'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweets_da\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mtweets_da\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'å'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'aa'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweets_da\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tweets_da' is not defined"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "\n",
    "# specify and extend stopwords\n",
    "stopwords_da = stopwords.words(\"danish\")  \n",
    "stopwords_da.extend(['paa', 'saa', 'vaere',  'rt', 'ogsaa', 'faa', 'faar', 'nok', 'mt', 'gt'])\n",
    "\n",
    "\n",
    "# exchange special characters æ, ø, å for their 'international' equivalents.\n",
    "tweets_da['clean_text'] = [re.sub('ø', 'oe', text) for text in tweets_da['comment_body']] # concatenated_text for posts and comment_body for comments\n",
    "tweets_da['clean_text'] = [re.sub('æ', 'ae', text) for text in tweets_da['clean_text']]\n",
    "tweets_da['clean_text'] = [re.sub('å', 'aa', text) for text in tweets_da['clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce0066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply cleaning function\n",
    "tweets_da['clean_text'] = clean_text(tweets_da['clean_text'], stops=stopwords_da)\n",
    "\n",
    "# apply tokenization\n",
    "tweets_da['clean_text'] = tokenize_text(tweets_da['clean_text'])\n",
    "\n",
    "# apply lemmatization\n",
    "lemmas = []\n",
    "for tweet in tweets_da['clean_text']:\n",
    "    lemma = [lemmatize_text(x) for x in tweet]\n",
    "    lemmas.append([item for sublist in lemma for item in sublist])\n",
    "tweets_da['lemmatized_text'] = lemmas\n",
    "\n",
    "# apply stemming\n",
    "stems = []\n",
    "for tweet in tweets_da['clean_text']:\n",
    "    stems.append(stem_text(tweet, langStemmer=DanishStemmer()))\n",
    "tweets_da['stemmed_text'] = stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee2dafb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets_da' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-73e0a11e0436>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtweets_da\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tweets_da' is not defined"
     ]
    }
   ],
   "source": [
    "tweets_da.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69484a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"C:/Users/stine/OneDrive/melchior_job/preprocessed/comments_new_female_nodups_prepro_lang.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
